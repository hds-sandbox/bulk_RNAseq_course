---
title: "The RNAseq count matrix"
author: "You!"
date: '`r Sys.Date()`'
knit: (function(inputFile, encoding) { 
      rmarkdown::render(inputFile,
                        encoding=encoding,
                        output_format='all',
                        output_dir='./')})
output:
  # To create PDF report, uncomment below
  #pdf_document:
  #  toc: yes  
  html_document:
    theme: yeti # nice theme for the webpage
    toc: yes # table of contents
    toc_float: yes # table of contents "floats" in the document
    df_print: paged # data frames are interactive
    dev: png # what format do you want for the figures?
---

```{r knitr, include = FALSE}
DOCNAME = knitr::current_input()
DOCNAME = gsub(DOCNAME, pattern = ".Rmd", replacement = "", fixed = T)
knitr::opts_chunk$set(autodep        = TRUE,
                      cache          = FALSE,
                      echo           = TRUE,
                      error          = FALSE,
                      fig.align      = "center",
                      fig.path       = paste0("./img/", DOCNAME, "/"), #images will be put in this folder, under the notebook name
                      message        = FALSE,
                      warning        = FALSE,
                      eval           = TRUE)
```

Approximate time: 20 minutes

## Learning Objectives

-   Load and create a count matrix from our preprocessing analysis using Salmon
-   Explain why negative binomial distribution is used to model RNA-seq count data

## Loading libraries

For this analysis we will be using several R packages, some which have been installed from CRAN and others from Bioconductor. To use these packages (and the functions contained within them), we need to **load the libraries.**

```{r}
library(tidyverse)
library(DESeq2)
library(tximport)

# And with this last line of code, we set our working directory to the folder with this notebook.
# This way, the relative paths will work without issues
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
```

## Loading data

The directories of output from the mapping/quantification step of the workflow (Salmon) is the data that we will be using. These transcript abundance estimates, often referred to as **'pseudocounts', will be the starting point for our differential gene expression analysis**. The main output of Salmon is a `quant.sf` file, and we have one of these for each individual sample in our dataset.

For the sake of reproducibility, we will be using the backup results from our preprocessing pipeline. You are welcome to use your own results!

```{r}
# Tabulated separated files can be opened using the read_table() function.
read_table("../Data/salmon/control_1/quant.sf") %>% head()
```

For each transcript that was assayed in the reference, we have:

1.  The transcript identifier
2.  The transcript length (in bp)
3.  The effective length (described in detail below)
4.  TPM (transcripts per million), which is computed using the effective length
5.  The estimated read count ('pseudocount')

> #### What exactly is the effective length?
>
> The sequence composition of a transcript affects how many reads are sampled from it. While two transcripts might be of identical actual length, depending on the sequence composition we are more likely to generate fragments from one versus the other. The transcript that has a higer likelihood of being sampled, will end up with the larger effective length. The effective length is transcript length which has been "corrected" to include factors due to sequence-specific and GC biases.

We will be using the R Bioconductor package `tximport` to prepare the `quant.sf` files for DESeq2. The first thing we need to do is create a variable that contains the paths to each of our `quant.sf` files. Then we will **add names to our quant files which will allow us to easily distinguish between samples in the final output matrix**.

We will use the `samplesheet.csv` file that we use to process our raw reads, since it already contains all the information we need to run our analysis.

```{r}
# Load metadata
meta <- read_csv("../Data/samplesheet.csv")

# View metadata
meta
```

Using the samples column, we can create all the paths needed:

```{r}
# Directory where salmon files are. You can change this path to the results of your own analysis
dir <- "../Data/salmon"

# List all directories containing quant.sf files using the samplename column of metadata
files <- file.path(dir, meta$sample, "quant.sf")

# Name the file list with the samplenames
names(files) <- meta$sample
files
```

Our Salmon files were generated with transcript sequences listed by Ensembl IDs, but `tximport` needs to know **which genes these transcripts came from**. We will use annotation table the that was created in our workflow, called `tx2gene.txt`.

```{r}
tx2gene <- read_table("../Data/salmon/salmon_tx2gene.tsv", col_names = c("transcript_ID","gene_ID","gene_symbol"))
tx2gene %>% head()
```

**`tx2gene`** is a three-column **data frame linking transcript ID (column 1) to gene ID (column 2)** to gene symbol (column 3). We will take the first two columns as input to `tximport`. The **column names are not relevant, but the column order is (i.e transcript ID must be first).**

Now we are ready to **run `tximport`**. The `tximport()` function imports transcript-level estimates from various external software (e.g. Salmon, Kallisto) and summarizes to the gene-level (default) or outputs transcript-level matrices. There are optional arguments to use the abundance estimates as they appear in the `quant.sf` files or to calculate alternative values.

For our analysis we **need non-normalized or "raw" count estimates at the gene-level for performing DESeq2 analysis**.

Since the gene-level count matrix is a default (`txOut=FALSE`) there is only one additional argument for us to modify **to specify how to obtain our "raw" count values**. The options for `countsFromAbundance` are as follows:

-   `no` (default): This will take the values in TPM (as our scaled values) and NumReads (as our "raw" counts) columns, and collapse it down to the gene-level.
-   `scaledTPM`: This is taking the TPM scaled up to library size as our "raw" counts
-   `lengthScaledTPM`: This is used to generate the "raw" count table from the TPM (rather than summarizing the NumReads column). "Raw" count values are generated by using the TPM value x featureLength x library size. These represent quantities that are on the same scale as original counts, except no longer correlated with transcript length across samples. **We will use this option for DESeq2 downstream analysis**.

**An additional argument for `tximport`**: When performing your own analysis you may find that the reference transcriptome file you obtain from Ensembl will have version numbers included on your identifiers (i.e ENSG00000265439.2). This will cause a discrepancy with the tx2gene file since the annotation databases don't usually contain version numbers (i.e ENSG00000265439). To get around this issue you can use the argument `ignoreTxVersion  = TRUE`. The logical value indicates whether to split the tx id on the '.' character to remove version information, for easier matching.

```{r}
txi <- tximport(files, type="salmon", tx2gene=tx2gene, countsFromAbundance = "lengthScaledTPM", ignoreTxVersion	= TRUE)
```

### Viewing data

The `txi` object is a simple list containing matrices of the abundance, counts, length. Another list element 'countsFromAbundance' carries through the character argument used in the tximport call. The length matrix contains the average transcript length for each gene which can be used as an offset for gene-level analysis.

```{r}
attributes(txi)
```

We will be using the `txi` object as is for input into DESeq2, but will save it until the next lesson. **For now let's take a look at the count matrix.** You will notice that there are decimal values, so let's round to the nearest whole number and convert it into a dataframe. We will save it to a variable called `data` that we can play with.

```{r}
# Look at the counts
txi$counts %>% head()
```

```{r}
# Write the counts to an object
data <- txi$counts %>% 
  round() %>% 
  data.frame()
```

There are a lot of rows with no gene expression at all.

```{r}
sum(rowSums(data) == 0)
```

Let's take them out.

```{r}
keep <- rowSums(data) > 0
data <- data[keep,]
```

## RNA-seq count distribution

To determine the appropriate statistical model, we need information about the distribution of counts. To get an idea about how RNA-seq counts are distributed, let's plot the counts of all the samples:

```{r}
# Here we format the data into long format instead of wide format
pdata <- data %>% 
  gather(key = Sample, value = Count)

pdata
```

And we plot our count distribution using all our samples:

```{r count_distribution}
ggplot(pdata) +
  geom_density(aes(x = Count, color = Sample)) +
  xlab("Raw expression counts") +
  ylab("Number of genes")
```

If we zoom in close to zero, we can see a large number of genes with counts close to zero:

```{r count_distribution_zoom}
ggplot(pdata) +
  geom_density(aes(x = Count, color = Sample)) +
  xlim(-5, 500)  +
  xlab("Raw expression counts") +
  ylab("Number of genes")
```

These images illustrate some common features of RNA-seq count data, including a **low number of counts associated with a large proportion of genes**, and a long right tail due to the **lack of any upper limit for expression**. Unlike microarray data, which has a dynamic range maximum limited due to when the probes max out, there is no limit of maximum expression for RNA-seq data. Due to the differences in these technologies, the statistical models used to fit the data are different between the two methods.

## Modeling count data

RNAseq count data can be modeled using a **Poisson distribution**. this particular distribution is fitting for data where the **number of cases is very large but the probability of an event occurring is very small**. To give you an example, think of the lottery: many people buy lottery tickets (high number of cases), but only very few win (the probability of the event is small).

With RNA-Seq data, **a very large number of RNAs are represented and the probability of pulling out a particular transcript is very small**. Thus, it would be an appropriate situation to use the Poisson distribution. However, a unique property of this distribution is that the mean == variance. Realistically, with RNA-Seq data there is always some biological variation present across the replicates (within a sample class). Genes with larger average expression levels will tend to have larger observed variances across replicates.

The model that fits best, given this type of variability observed for replicates, is the **Negative Binomial (NB) model**. Essentially, **the NB model is a good approximation for data where the mean \< variance**, as is the case with RNA-Seq count data.

Here we calculate the mean and the variance per gene for all columns and genes:

```{r}
means <- rowMeans(data, na.rm = TRUE)
variances <- rowVars(as.matrix(data), na.rm = TRUE)

# Create a new dataframe 'df' to store the results
df <- data.frame(rownames = rownames(data), mean_counts = means, variance_counts = variances)
```

Run the following code to plot the *mean versus variance* of each gene for our data:

```{r mean_vs_variance}
ggplot(df) +
  geom_point(aes(x=mean_counts, y=variance_counts)) + 
  geom_abline(intercept = 0, slope = 1, color="red") +
  scale_y_log10() +
  scale_x_log10()
```

If the mean would be equal to the variance, the cloud of points would follow the straight red line.
